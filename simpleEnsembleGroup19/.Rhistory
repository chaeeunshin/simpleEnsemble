data$y <- as.factor(data$y)
nrow(train_data)
nrow(test_data)
model <- glm(y~., train_data, family=binomial)
probabilities <- model %>% predict(test_data,type="response")
predicted_class_lm <- ifelse(probabilities>0.5,1,0)
confusionMatrix(factor(predicted_class_lm),factor(test_data$y))
mydata <- cbind(test_data$y,predicted_class_lm)
set.seed(123)
model <- neuralnet(y~., data=train_data, hidden=2,err.fct="sse",linear.output=F)
plot(model, rep="best")
probabilities <- model %>% predict(test_data) %>% as.vector()
predicted_class_sse_2 <- ifelse(probabilities>0.5,1,0)
confusionMatrix(factor(predicted_class_sse_2),factor(test_data$y),positive='1')
set.seed(123)
model <- neuralnet(y~., data=train_data, hidden=3,err.fct="sse",linear.output=F)
plot(model, rep="best")
probabilities <- model %>% predict(test_data) %>% as.vector()
predicted_class_sse_3 <- ifelse(probabilities>0.5,1,0)
confusionMatrix(factor(predicted_class_sse_3),factor(test_data$y),positive='1')
set.seed(123)
model <- neuralnet(y~., data=train_data, hidden=4,err.fct="sse",linear.output=F)
plot(model, rep="best")
probabilities <- model %>% predict(test_data) %>% as.vector()
predicted_class_sse_4 <- ifelse(probabilities>0.5,1,0)
confusionMatrix(factor(predicted_class_sse_4),factor(test_data$y),positive='1')
set.seed(123)
model <- neuralnet(y~., data=train_data, hidden=2,err.fct="ce",linear.output=F)
plot(model, rep="best")
probabilities <- model %>% predict(test_data) %>% as.vector()
predicted_class_ce_2 <- ifelse(probabilities>0.5,1,0)
confusionMatrix(factor(predicted_class_ce_2),factor(test_data$y),positive='1')
set.seed(123)
model <- neuralnet(y~., data=train_data, hidden=3,err.fct="ce",linear.output=F)
plot(model, rep="best")
probabilities <- model %>% predict(test_data) %>% as.vector()
predicted_class_ce_3 <- ifelse(probabilities>0.5,1,0)
confusionMatrix(factor(predicted_class_ce_3),factor(test_data$y),positive='1')
set.seed(123)
model <- neuralnet(y~., data=train_data, hidden=4,err.fct="ce",linear.output=F)
plot(model, rep="best")
probabilities <- model %>% predict(test_data) %>% as.vector()
predicted_class_ce_4 <- ifelse(probabilities>0.5,1,0)
confusionMatrix(factor(predicted_class_ce_4),factor(test_data$y),positive='1')
mydata <- cbind(mydata,predicted_class_sse_2 )
model <- rpart(y~., data=train_data, control=rpart.control(cp=0))
fancyRpartPlot(model)
set.seed(123)
model2 <- train(y~., data=train_data, method="rpart",trControl=trainControl("cv",number=10),tuneLength=100)
plot(model2)
model2$bestTune
fancyRpartPlot(model2$finalModel)
probabilities <- predict(model2, newdata=test_data)
predicted_class_prunedtree <- ifelse(probabilities>0.5,1,0)
confusionMatrix(factor(predicted_class_prunedtree),factor(test_data$y))
mydata <- cbind(mydata, predicted_class_prunedtree)
train_data$y <- factor(train_data$y)
test_data$y <- factor(test_data$y)
set.seed(123)
model <- train(y~., data=train_data, method="rf",trControl=trainControl("cv",number=10),importance=TRUE)
model$finalModel
predicted_class_rf <- model %>% predict(test_data)
confusionMatrix(predicted_class_rf, test_data$y, positive='1')
set.seed(123)
model <- train(y~., data=train_data, method="svmRadial",trControl=trainControl("cv",number=10),tuneLength=4)
plot(model)
model$bestTune
predicted_class_svmPoly <- predict(model, newdata=test_data)
confusionMatrix(predicted_class_svmPoly, test_data$y)
mydata <- cbind(mydata,predicted_class_svmPoly)
pred.m <- apply(mydata,1,function(x) names(which.max(table(x))))
pred.m <- factor(pred.m, levels=c('1','2'),labels=c('0','1'))
confusionMatrix(pred.m, test.data$y, positive='1')
pred.m <- apply(mydata,1,function(x) names(which.max(table(x))))
pred.m <- factor(pred.m, levels=c('1','2'),labels=c('0','1'))
confusionMatrix(pred.m, test_data$y, positive='1')
pred.m <- apply(mydata,1,function(x) names(which.max(table(x))))
pred.m
confusionMatrix(pred.m, test_data$y, positive='1')
pred.m <- apply(mydata,1,function(x) names(which.max(table(x))))
confusionMatrix(factor(pred.m), test_data$y, positive='1')
data <- read.csv('restt.csv')
ggplot(data,aes(x=Date,y=Close)) + geom_line()
data
data$min_lagged <- lag(data$Low)
data$max_lagged <- log(data$High)
data$Close_norm <- (data$Close-data$min_lagged)/(data$max_lagged-data$min_lagged)
model_data <- matrix(data$Close_norm[-1])
knitr::kable(tail(model_data,10))
data <- read.csv('restt.csv')
data <- na.omit(data)
ggplot(data,aes(x=Date,y=Close)) + geom_line()
data$min_lagged <- lag(data$Low)
data$max_lagged <- log(data$High)
data$Close_norm <- (data$Close-data$min_lagged)/(data$max_lagged-data$min_lagged)
model_data <- matrix(data$Close_norm[-1])
knitr::kable(tail(model_data,10))
train_data <- head(model_data,-5)
test_data <- tail(model_data,10)
cat(dim(train_data)[1], 'days are divided into the training set')
train_data <- head(model_data,-10)
test_data <- tail(model_data,10)
cat(dim(train_data)[1], 'days are divided into the training set')
prediction <- 10
lag = prediction
train_X = t(sapply(1:(length(train_data) - lag - prediction + 1),function(x) train_data[x:(x + lag - 1), 1]))
train_X <- array(data = as.numeric(unlist(train_X)),dim = c(nrow(train_X),lag,1))
# Training y
train_y <- t(sapply((1 + lag):(length(train_data) - prediction + 1),function(x) train_data[x:(x + prediction - 1)]))
train_y <- array(data = as.numeric(unlist(train_y)),dim = c(nrow(train_y),prediction,1))
# Testing X
test_X = t(sapply(1:(length(test_data) - lag - prediction + 1),function(x) test_data[x:(x + lag - 1), 1]))test_X <- array(data = as.numeric(unlist(test_X)),dim = c(nrow(test_X),lag,151))
prediction <- 10
lag = prediction
train_X = t(sapply(1:(length(train_data) - lag - prediction + 1),function(x) train_data[x:(x + lag - 1), 1]))
train_X <- array(data = as.numeric(unlist(train_X)),dim = c(nrow(train_X),lag,1))
# Training y
train_y <- t(sapply((1 + lag):(length(train_data) - prediction + 1),function(x) train_data[x:(x + prediction - 1)]))
train_y <- array(data = as.numeric(unlist(train_y)),dim = c(nrow(train_y),prediction,1))
# Testing X
test_X = t(sapply(1:(length(test_data) - lag - prediction + 1),function(x) test_data[x:(x + lag - 1), 1]))
prediction <- 10
lag = prediction
train_X = t(sapply(1:(length(train_data) - lag - prediction + 1),function(x) train_data[x:(x + lag - 1), 1]))
train_X <- array(data = as.numeric(unlist(train_X)),dim = c(nrow(train_X),lag,1))
# Training y
train_y <- t(sapply((1 + lag):(length(train_data) - prediction + 1),function(x) train_data[x:(x + prediction - 1)]))
train_y <- array(data = as.numeric(unlist(train_y)),dim = c(nrow(train_y),prediction,1))
# Testing X
test_X = t(sapply(1:(length(test_data) - lag - prediction + 1), function(x) test_data[x:(x + lag - 1), 1]))
data <- read.csv("rest.csv")
data <- na.omit(data)
k.means.fit <- kmeans(data,2)
library(cluster)
clusplot(data,k.means.fit$cluster,main="2D representation of the Cluster solution",color=TRUE,shade=TRUE,labels=1,lines=0)
table(k.means.fit$cluster,data$y)
data <- read.csv("rest.csv")
data <- na.omit(data)
k.means.fit <- kmeans(data,2)
library(cluster)
clusplot(data,k.means.fit$cluster,main="2D representation of the Cluster solution",color=TRUE,shade=TRUE,labels=1,lines=0)
table(k.means.fit$cluster,data$y)
(370+7)/(366+2422+1806+7)
d <- dist(data,method="euclidean")
H.fit <- hclust(d,method="ward.D")
plot(H.fit)
rect.hclust(H.fit,k=2,border="red")
groups <- cutree(H.fit,k=2)
clusters.ward <- factor(groups,levels=1:2,labels=c(0,1))
clusplot(data,groups,main="2D representation of the Cluster solution",color=TRUE,shade=TRUE,labels=2,lines=0)
table(data$y,clusters.ward)
(2191+2)/(2191+597+1811+2)
table(k.means.fit$cluster,clusters.ward)
H.fit <- hclust(d,method="average")
plot(H.fit)
rect.hclust(H.fit,k=2,border="red")
groups <- cutree(H.fit,k=2)
clusters.average <- factor(groups,levels=1:2,labels=c(0,1))
clusplot(data,groups,main="2D representation of the Cluster solution",color=TRUE,shade=TRUE,labels=2,lines=0)
table(data$y,clusters.average)
(2786+1)/(1812+2+1+2786)
pred <- cbind(predicted_class_sse_2, predicted_class_rf, predicted_class_prunedtree)
pred.m <- apply(mydata,1,function(x) names(which.max(table(x))))
confusionMatrix(factor(pred.m), test_data$y, positive='1')
pred <- cbind(predicted_class_sse_2, predicted_class_rf, predicted_class_prunedtree)
pred.m <- apply(mydata,1,function(x) names(which.max(table(x))))
confusionMatrix(factor(pred.m), test_data[,12], positive='1')
if(!requireNamespace("tidyverse")) install.packages("tidyverse")
if(!requireNamespace("caret")) install.packages("caret")
if(!requireNamespace("neuralnet")) install.packages("neuralnet")
if(!requireNamespace("keras")) install.packages("keras")
if(!requireNamespace("randomForest")) install.packages("randomForest")
if(!requireNamespace("rpart")) install.packages("rpart")
if(!requireNamespace("rattle")) install.packages("rattle")
if(!requireNamespace("kernlab")) install.packages("kernlab")
library(tidyverse)
library(caret)
library(neuralnet)
library(keras)
library(randomForest)
library(rpart)
library(rattle)
library(kernlab)
library(MASS)
library(tidyverse)
library(glmnet)
library(leaps)
library(ggplot2)
data <- read.csv('rest.csv')
data <- na.omit(data)
cat("There are", dim(data)[1], "observations left.")
set.seed(123)
training.samples <- data$y %>%
createDataPartition(p=0.75,list=FALSE)
train_data <- data[training.samples,]
test_data <- data[-training.samples,]
data$y <- as.factor(data$y)
nrow(train_data)
nrow(test_data)
model <- glm(y~., train_data, family=binomial)
probabilities <- model %>% predict(test_data,type="response")
predicted_class_lm <- ifelse(probabilities>0.5,1,0)
confusionMatrix(factor(predicted_class_lm),factor(test_data$y))
set.seed(123)
model <- neuralnet(y~., data=train_data, hidden=2,err.fct="sse",linear.output=F)
plot(model, rep="best")
probabilities <- model %>% predict(test_data) %>% as.vector()
predicted_class_sse_2 <- ifelse(probabilities>0.5,1,0)
confusionMatrix(factor(predicted_class_sse_2),factor(test_data$y),positive='1')
set.seed(123)
model <- neuralnet(y~., data=train_data, hidden=3,err.fct="sse",linear.output=F)
plot(model, rep="best")
probabilities <- model %>% predict(test_data) %>% as.vector()
predicted_class_sse_3 <- ifelse(probabilities>0.5,1,0)
confusionMatrix(factor(predicted_class_sse_3),factor(test_data$y),positive='1')
set.seed(123)
model <- neuralnet(y~., data=train_data, hidden=4,err.fct="sse",linear.output=F)
plot(model, rep="best")
probabilities <- model %>% predict(test_data) %>% as.vector()
predicted_class_sse_4 <- ifelse(probabilities>0.5,1,0)
confusionMatrix(factor(predicted_class_sse_4),factor(test_data$y),positive='1')
set.seed(123)
model <- neuralnet(y~., data=train_data, hidden=2,err.fct="ce",linear.output=F)
plot(model, rep="best")
probabilities <- model %>% predict(test_data) %>% as.vector()
predicted_class_ce_2 <- ifelse(probabilities>0.5,1,0)
confusionMatrix(factor(predicted_class_ce_2),factor(test_data$y),positive='1')
set.seed(123)
model <- neuralnet(y~., data=train_data, hidden=3,err.fct="ce",linear.output=F)
plot(model, rep="best")
probabilities <- model %>% predict(test_data) %>% as.vector()
predicted_class_ce_3 <- ifelse(probabilities>0.5,1,0)
confusionMatrix(factor(predicted_class_ce_3),factor(test_data$y),positive='1')
set.seed(123)
model <- neuralnet(y~., data=train_data, hidden=4,err.fct="ce",linear.output=F)
plot(model, rep="best")
probabilities <- model %>% predict(test_data) %>% as.vector()
predicted_class_ce_4 <- ifelse(probabilities>0.5,1,0)
confusionMatrix(factor(predicted_class_ce_4),factor(test_data$y),positive='1')
model <- rpart(y~., data=train_data, control=rpart.control(cp=0))
fancyRpartPlot(model)
set.seed(123)
model2 <- train(y~., data=train_data, method="rpart",trControl=trainControl("cv",number=10),tuneLength=100)
plot(model2)
model2$bestTune
fancyRpartPlot(model2$finalModel)
probabilities <- predict(model2, newdata=test_data)
predicted_class_prunedtree <- ifelse(probabilities>0.5,1,0)
confusionMatrix(factor(predicted_class_prunedtree),factor(test_data$y))
train_data$y <- factor(train_data$y)
test_data$y <- factor(test_data$y)
set.seed(123)
model <- train(y~., data=train_data, method="rf",trControl=trainControl("cv",number=10),importance=TRUE)
model$finalModel
(1984+1168)/(1984+110+189+1168)
1168/(1168+189)
1984/(1984+110)
predicted_class_rf <- model %>% predict(test_data)
confusionMatrix(predicted_class_rf, test_data$y, positive='1')
varImpPlot(model$finalModel, type=1)
varImpPlot(model$finalModel, type=2)
varImp(model,type=2)
set.seed(123)
model <- train(y~., data=train_data, method="svmRadial",trControl=trainControl("cv",number=10),tuneLength=4)
plot(model)
model$bestTune
predicted_class_svmPoly <- predict(model, newdata=test_data)
confusionMatrix(predicted_class_svmPoly, test_data$y)
pred <- cbind(predicted_class_sse_2, predicted_class_rf, predicted_class_prunedtree)
pred.m <- apply(mydata,1,function(x) names(which.max(table(x))))
confusionMatrix(factor(pred.m), test_data[,12], positive='1')
pred <- cbind(predicted_class_sse_2, predicted_class_rf, predicted_class_prunedtree)
pred.m <- apply(mydata,1,function(x) names(which.max(table(x))))
confusionMatrix(factor(pred.m), test_data$y, positive='1')
data <- read.csv("rest.csv")
data <- na.omit(data)
k.means.fit <- kmeans(data,2)
library(cluster)
clusplot(data,k.means.fit$cluster,main="2D representation of the Cluster solution",color=TRUE,shade=TRUE,labels=1,lines=0)
table(k.means.fit$cluster,data$y)
(370+7)/(366+2422+1806+7)
d <- dist(data,method="euclidean")
H.fit <- hclust(d,method="ward.D")
plot(H.fit)
rect.hclust(H.fit,k=2,border="red")
groups <- cutree(H.fit,k=2)
clusters.ward <- factor(groups,levels=1:2,labels=c(0,1))
clusplot(data,groups,main="2D representation of the Cluster solution",color=TRUE,shade=TRUE,labels=2,lines=0)
table(data$y,clusters.ward)
(2191+2)/(2191+597+1811+2)
table(k.means.fit$cluster,clusters.ward)
H.fit <- hclust(d,method="average")
plot(H.fit)
rect.hclust(H.fit,k=2,border="red")
groups <- cutree(H.fit,k=2)
clusters.average <- factor(groups,levels=1:2,labels=c(0,1))
clusplot(data,groups,main="2D representation of the Cluster solution",color=TRUE,shade=TRUE,labels=2,lines=0)
table(data$y,clusters.average)
(2786+1)/(1812+2+1+2786)
library(devtools)
library(ggbiplot)
data.pca <- prcomp(data,center=TRUE,scale. = TRUE)
summary(data.pca)
ggbiplot(data.pca)
ggbiplot(data.pca, ellipse = TRUE, groups = data$y)
data.pca$rotation[,1]
d <- dist(data,method="euclidean")
H.fit <- hclust(d,method="ward.D")
plot(H.fit)
rect.hclust(H.fit,k=2,border="red")
groups <- cutree(H.fit,k=2)
clusters.ward <- factor(groups,levels=1:2,labels=c(0,1))
clusplot(data,groups,main="2D representation of the Cluster solution",color=TRUE,shade=TRUE,labels=2,lines=0)
table(data$y,clusters.ward)
k.means.fit <- kmeans(data,2)
library(cluster)
clusplot(data,k.means.fit$cluster,main="2D representation of the Cluster solution",color=TRUE,shade=TRUE,labels=1,lines=0)
table(k.means.fit$cluster,data$y)
prediction = 10
lag = prediction
train_X <- t(sapply(1:(length(train_data)-lag-prediction+1),function(x) train_data[x:(x+lag-1),1]))
if(!requireNamespace("tidyquant")) install.packages("tidyquant")
if(!requireNamespace("magrittr")) install.packages("magrittr")
if(!requireNamespace("tensorflow")) install.packages("tensorflow")
if(!requireNamespace("zoo")) install.packages("zoo")
library(tidyquant)
library(magrittr)
library(zoo)
data <- read.csv('restt.csv')
data <- na.omit(data)
ggplot(data,aes(x=Date,y=Close)) + geom_line()
data$min_lagged <- lag(data$Low)
data$max_lagged <- log(data$High)
data$Close_norm <- (data$Close-data$min_lagged)/(data$max_lagged-data$min_lagged)
model_data <- matrix(data$Close_norm[-1])
knitr::kable(tail(model_data,10))
train_data <- head(model_data,-10)
test_data <- tail(model_data,10)
cat(dim(train_data)[1], 'days are divided into the training set')
prediction = 5
lag = prediction
train_X <- t(sapply(1:(length(train_data)-lag-prediction+1),function(x) train_data[x:(x+lag-1),1]))
prediction = 5
lag = prediction
train_X <- t(sapply(1:(length(train_data)-lag-prediction+1),function(x) train_data[x:(x+lag-1),1]))
train_X <- array(data=as.numeric(unlist(train_X)),dim=c(nrow(train_X),lag,1))
train_y <- t(sapply((1 + lag):(length(train_data) - prediction + 1),function(x) train_data[x:(x + prediction - 1)]))
train_y <- array(data = as.numeric(unlist(train_y)),dim = c(nrow(train_y),prediction,1))
prediction = 5
lag = prediction
train_X <- t(sapply(1:(length(train_data)-lag-prediction+1),function(x) train_data[x:(x+lag-1),1]))
train_X <- array(data=as.numeric(unlist(train_X)),dim=c(nrow(train_X),lag,1))
train_y <- t(sapply((1 + lag):(length(train_data) - prediction + 1),function(x) train_data[x:(x + prediction - 1)]))
train_y <- array(data = as.numeric(unlist(train_y)),dim = c(nrow(train_y),prediction,1))
# Testing X
test_X = t(sapply(
1:(length(test_data) - lag - prediction + 1),
function(x) test_data[x:(x + lag - 1), 1]
))
test_X <- array(
data = as.numeric(unlist(test_X)),
dim = c(
nrow(test_X),
lag,
15
1
setwd("Users/chaeeunshin/Desktop/AMS 597 Group Project/simpleEnsembleGroup19")
setwd("/Users/chaeeunshin/Desktop/AMS 597 Group Project/simpleEnsembleGroup19")
devtools::document()
devtools::build()
simpleEnsembleGroup19::baggingLogistic(bin.x,bin.y)
library(simpleEnsembleGroup19)
baggingLogistic(simpleEnsembleGroup19::bin.x,simpleEnsembleGroup19::bin.y,0)
help(simpleEnsembleGroup19::baggingLogistic)
help(simpleEnsembleGroup19)
??simpleEnsembleGroup19
help(simpleEnsembleGroup19::baggingPrediction)
devtools::document()
devtools::build()
help(simpleEnsembleGroup19::baggingPrediction)
simpleEnsembleGroup19::ensembleLearning(simpleEnsembleGroup19::cont.x,simpleEnsembleGroup19::cont.y)
modelPrediction(simpleEnsembleGroup19::cont.x,simpleEnsembleGroup19::cont.y)
baggingPrediction(simpleEnsembleGroup19::cont.x,simpleEnsembleGroup19::cont.y)
load("~/Desktop/AMS 597 Group Project/simpleEnsembleGroup19/data/bin.x.rda")
load("~/Desktop/AMS 597 Group Project/simpleEnsembleGroup19/data/bin.y.rda")
modelPrediction(bin.x,bin.y)
baggingPrediction(bin.x,bin.y,R=100)
ensembleLearning(bin.x,bin.y)
is.integer(2)
is.integer('3')
class(2)
is.numeric(3)
is.numeric(2,3)
is.numeric(2.3)
is.integer(1.-)
is.integer(1.0)
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
baggingRidge(bin.x,bin.y,50)
devtools::document()
devtools::build()
bagginLogistic(bin.x,bin.y,50,0.8)
baggingLogistic(bin.x,bin.y,50,0.8)
devtools::document()
devtools::build()
baggingLogistic(bin.x,bin.y,50,0.8)
#' This function performs bootstrap bagging R times. R must be given by users.
#' @param X A matrix of predictor variable
#' @param y A vector of response variable
#' @param R The number of bootstrap replicates to perform
#' @param p The proportion of train data
#' @return This function returns final.model, which is final bagged model.
#' @export
baggingLogistic <- function(X,y,R,p) {
result <- check_data(X,y)
traintest <- train_test(X,y,p)
train_X <- traintest$train_X
train_y <- traintest$train_y
test_X <- traintest$test_X
test_y <- traintest$test_y
models <- list()
predictions <- list()
if (R<=1 || is.na(R)==TRUE)
stop("Invalid input of R. R has to be positive integer greater than 1")
if (result$response_type=="continuous")
stop("Not valid response variable. Response variable must be binary for logistic regression.")
else {
for(i in 1:R) {
sample_indicies <- sample(1:nrow(train_X),nrow(train_X),replace=TRUE)
X_samples <- train_X[sample_indicies,]
y_samples <- train_y[sample_indicies]
models[[i]] <- fitLogisticModel(X_samples,y_samples)
predictions[[i]] <- predict(models[[i]],newx=test_X, type="response")
}
final.predictions <- rowMeans(do.call(cbind,predictions))
cm <- table(final.predictions,test_y)
accuracy <- (cm[1,1]+cm[2,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
}
return(list(confusionMatrix=cm, accuracy=accuracy))
}
baggingLogistic(bin.x,bin.y,50,0.8)
devtool::document()
devtools::document()
baggingLogistic(bin.x,bin.y,50,0.8)
#' This function performs bootstrap bagging R times. R must be given by users.
#' @param X A matrix of predictor variable
#' @param y A vector of response variable
#' @param R The number of bootstrap replicates to perform
#' @param p The proportion of train data
#' @return This function returns final.model, which is final bagged model.
#' @export
baggingLogistic <- function(X,y,R,p) {
result <- check_data(X,y)
traintest <- train_test(X,y,p)
train_X <- traintest$train_X
train_y <- traintest$train_y
test_X <- traintest$test_X
test_y <- traintest$test_y
models <- list()
predictions <- list()
if (R<=1 || is.na(R)==TRUE)
stop("Invalid input of R. R has to be positive integer greater than 1")
if (result$response_type=="continuous")
stop("Not valid response variable. Response variable must be binary for logistic regression.")
else {
for(i in 1:R) {
sample_indicies <- sample(1:nrow(train_X),nrow(train_X),replace=TRUE)
X_samples <- train_X[sample_indicies,]
y_samples <- train_y[sample_indicies]
models[[i]] <- fitLogisticModel(X_samples,y_samples)
predictions[[i]] <- cbind(1,test_X) %*% coef(models[[i]])
}
final.predictions <- rowMeans(do.call(cbind,predictions))
final.predictions <- ifelse(final.predictions>0.5,1,0)
cm <- table(final.predictions,test_y)
accuracy <- (cm[1,1]+cm[2,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
}
return(list(confusionMatrix=cm, accuracy=accuracy))
}
baggingLogistic(bin.x,bin.y,50,0.8)
#' This function performs bootstrap bagging R times. R must be given by users.
#' @param X A matrix of predictor variables
#' @param y A vector of response variable
#' @param R The number of bootstrap replicates to perform
#' @return This function returns final.model, which is final bagged model.
#' @export
baggingLinear <- function(X,y,R) {
result <- check_data(X,y)
models <- list()
models.coef <- list()
if (R<=1 || is.na(R)==TRUE)
stop("Invalid input of R. R must be an integer greater than 1.")
if (result$response_type=="binary") {
stop("Not valid response variable. Response variable must be continuous for linear regression.")
}
else {
for(i in 1:R) {
sample_indicies <- sample(1:nrow(X),nrow(X),replace=TRUE)
X_samples <- X[sample_indicies,]
y_samples <- y[sample_indicies]
models[[i]] <- fitLinearModel(X_samples,y_samples)
predictions[[i]] <- cbind(1,test_X) %*% coef(models[[i]])
}
final.predictions <- rowMeans(do.call(cbind,predictions))
RMSE <- sqrt(mean(test_y-final.predictions)^2)
}
return(RMSE)
}
